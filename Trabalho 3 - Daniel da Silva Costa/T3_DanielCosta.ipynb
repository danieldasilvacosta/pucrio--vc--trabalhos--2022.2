{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Trabalho 3\n",
        "Descrição do trabalho:\n",
        "\n",
        "http://webserver2.tecgraf.puc-rio.br/~mgattass/visao/trb/T3.html\n",
        "\n",
        "Aluno: Daniel da Silva Costa\n",
        "\n",
        "E-mail: danieldasilvacosta@gmail.com"
      ],
      "metadata": {
        "id": "yRPDSJgl5066"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Imports"
      ],
      "metadata": {
        "id": "kXpKGbGz8FJ7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np"
      ],
      "metadata": {
        "id": "O9jx1Ztl8ID7"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Usando o Numpy"
      ],
      "metadata": {
        "id": "VGmsgRtP8283"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Funções de ativação"
      ],
      "metadata": {
        "id": "Y8fXUF_2-lVY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def tanh(x):\n",
        "  return np.tanh(x)\n",
        "\n",
        "def tanh_prime(x):\n",
        "  return 1-np.tanh(x)**2"
      ],
      "metadata": {
        "id": "ejfGb6st9Cai"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def sigmoid(x):\n",
        "  return 1/(1+np.exp(-x))\n",
        "\n",
        "def sigmoid_prime(x):\n",
        "  s = 1/(1+np.exp(-x))\n",
        "  ds = s*(1-s)\n",
        "  return ds"
      ],
      "metadata": {
        "id": "CnkdF7aw9CjL"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def relu(x):\n",
        "  return np.maximum(0,x)\n",
        "\n",
        "def relu_prime(x):\n",
        "  return (x>0).astype(x.dtype)"
      ],
      "metadata": {
        "id": "Anph_pJR9CqE"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Função loss"
      ],
      "metadata": {
        "id": "A2X3VCHE-iYN"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "UYJmw7CS5xBR"
      },
      "outputs": [],
      "source": [
        "# loss function and its derivative\n",
        "def mse(y_true, y_pred):\n",
        "  return np.mean(np.power(y_true-y_pred, 2))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def mse_prime(y_true, y_pred):\n",
        "  grad_output = 2*(y_pred-y_true)/y_true.size\n",
        "  return grad_output"
      ],
      "metadata": {
        "id": "pVna32JL55C6"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### class Layer"
      ],
      "metadata": {
        "id": "My5ulsha-o5q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Base class\n",
        "class Layer:\n",
        "  def __init__(self):\n",
        "    self.input = None\n",
        "    self.output = None\n",
        "\n",
        "  # computes the output Y of a layer for a given input X\n",
        "  def forward(self, input):\n",
        "    raise NotImplementedError\n",
        "\n",
        "  # computes dE/dX for a given dE/dY (and update parameters, if any)\n",
        "  def backward(self, grad_output, learning_rate):\n",
        "    raise NotImplementedError"
      ],
      "metadata": {
        "id": "oON88LkY55FY"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### class FCLayer(Layer)"
      ],
      "metadata": {
        "id": "-vqjZCnq-sUa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# inherit from base class Layer\n",
        "class FCLayer(Layer):\n",
        "  # input_size = number of input neurons\n",
        "  # output_size = number of output neurons\n",
        "  def __init__(self, input_size, output_size):\n",
        "    self.weights = 2*np.random.rand(input_size, output_size) - 1.0\n",
        "    self.bias = 2*np.random.rand(1, output_size) - 1.0\n",
        "\n",
        "  # returns output for a given input\n",
        "  def forward(self, input):\n",
        "    self.input = input\n",
        "    self.output = np.dot(input, self.weights) + self.bias\n",
        "    return self.output\n",
        "\n",
        "  # computes dE/dW, dE/dB for a given grad_output=dE/dY.\n",
        "  # returns grad_input=dE/dX.\n",
        "  # ajusta os pesos e bias em função da regra do\n",
        "  # gradiente descendente\n",
        "  def backward(self, grad_output, learning_rate):\n",
        "    grad_input = np.dot(grad_output, self.weights.T)\n",
        "    grad_weights = np.dot(self.input.T, grad_output)\n",
        "    grad_bias = grad_output\n",
        "\n",
        "    # update parameters\n",
        "    self.weights -= learning_rate * grad_weights\n",
        "    self.bias -= learning_rate * grad_bias\n",
        "    return grad_input"
      ],
      "metadata": {
        "id": "oTyVVx0A55J-"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### class ActivationLayer(Layer)"
      ],
      "metadata": {
        "id": "1jq0yFhG-ywV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class ActivationLayer(Layer):\n",
        "  def __init__(self, activation, activation_prime):\n",
        "    self.activation = activation\n",
        "    self.activation_prime = activation_prime\n",
        "\n",
        "  # returns the activated input\n",
        "  def forward(self, input):\n",
        "    self.input = input\n",
        "    self.output = self.activation(self.input)\n",
        "    return self.output\n",
        "\n",
        "  # Returns grad_input=dE/dX given grad_output=dE/dY.\n",
        "  # learning_rate is not used because there is no \"learnable\" parameters.\n",
        "  def backward(self, grad_output, learning_rate):\n",
        "    return self.activation_prime(self.input) * grad_output"
      ],
      "metadata": {
        "id": "yok3m0ub55QJ"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### class Network"
      ],
      "metadata": {
        "id": "a7r1j-lv-1nj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Network:\n",
        "  def __init__(self):\n",
        "    self.layers = []\n",
        "    self.loss = None\n",
        "    self.loss_prime = None\n",
        "\n",
        "  # add layer to network\n",
        "  def add(self, layer):\n",
        "    self.layers.append(layer)\n",
        "\n",
        "  # set loss to use\n",
        "  def use(self, loss, loss_prime):\n",
        "    self.loss = loss\n",
        "    self.loss_prime = loss_prime\n",
        "\n",
        "  # predict output for given input\n",
        "  def predict(self, input_data):\n",
        "    # sample dimension first\n",
        "    samples = len(input_data)\n",
        "    result = []\n",
        "    \n",
        "    # run network over all samples\n",
        "    for i in range(samples):\n",
        "    \n",
        "      # forward propagation\n",
        "      output = input_data[i]\n",
        "      for layer in self.layers:\n",
        "        output = layer.forward_propagation(output)\n",
        "        result.append(output)\n",
        "    return result\n",
        "\n",
        "  # train the network\n",
        "  def fit(self, x_train, y_train, epochs, learning_rate):\n",
        "    n_samples = x_train.shape[0]\n",
        "    \n",
        "    # training loop\n",
        "    for i in range(epochs):\n",
        "      err = 0\n",
        "      for j in range(n_samples):\n",
        "\n",
        "        # forward propagation\n",
        "        output = x_train[j]\n",
        "        for layer in self.layers:\n",
        "          output = layer.forward (output)\n",
        "\n",
        "        # compute loss (for display purpose only)\n",
        "        err += self.loss(y_train[j], output)\n",
        "\n",
        "        # backward propagation\n",
        "        grad_out = self.loss_prime(y_train[j], output)\n",
        "        for layer in reversed(self.layers):\n",
        "          grad_out = layer.backward(grad_out, learning_rate)\n",
        "\n",
        "      # calculate average error on all samples\n",
        "      err /= n_samples\n",
        "      print('epoch {i+1}/{epochs} error={err:.3f}')"
      ],
      "metadata": {
        "id": "1evNVPXU55Wr"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Treinamento"
      ],
      "metadata": {
        "id": "MfNBMV0I-3Fd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x_train = np.array( [1, 2, 3, 4, 5] )\n",
        "y_train = np.array( [2, 4, 6, 8, 10] )"
      ],
      "metadata": {
        "id": "AX3_DSCc55ZF"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "network = Network()\n",
        "input_size = len(x_train)\n",
        "output_size = 7 # output_size = m = number of weights\n",
        "network.add( FCLayer(input_size, output_size) )\n",
        "network.add( ActivationLayer(sigmoid, sigmoid_prime) )\n",
        "network.use( mse, mse_prime )\n",
        "network.fit(x_train, \n",
        "            y_train, \n",
        "            epochs = 10, \n",
        "            learning_rate = 0.001)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 328
        },
        "id": "jMxtReIr-YGf",
        "outputId": "42b0eda1-0eb5-468d-97c0-f0285ae587be"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-12-3058c494a260>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      8\u001b[0m             \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m             \u001b[0mepochs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m             learning_rate = 0.001)\n\u001b[0m",
            "\u001b[0;32m<ipython-input-10-9d81691848b2>\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x_train, y_train, epochs, learning_rate)\u001b[0m\n\u001b[1;32m     50\u001b[0m         \u001b[0mgrad_out\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss_prime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_train\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mlayer\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mreversed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 52\u001b[0;31m           \u001b[0mgrad_out\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlayer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrad_out\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlearning_rate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     53\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m       \u001b[0;31m# calculate average error on all samples\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-8-8c6eb11a9b63>\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, grad_output, learning_rate)\u001b[0m\n\u001b[1;32m     24\u001b[0m     \u001b[0;31m# update parameters\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweights\u001b[0m \u001b[0;34m-=\u001b[0m \u001b[0mlearning_rate\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mgrad_weights\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m \u001b[0;34m-=\u001b[0m \u001b[0mlearning_rate\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mgrad_bias\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mgrad_input\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: non-broadcastable output operand with shape (1,7) doesn't match the broadcast shape (5,7)"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "rw9ySzug-YI9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "qpxbBw2B-YLc"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}